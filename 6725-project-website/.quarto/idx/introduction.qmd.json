{"title":"Introduction","markdown":{"yaml":{"title":"Introduction"},"headingText":"AI-Enhanced E-commerce Data Engineering Project Introduction","containsRefs":false,"markdown":"\n\nTeam Members: Yuer Wang, Zining Wang, Jiatong Liu, Xue Qin\n\n## Abstract \n\nAs e-commerce continues to grow rapidly, massive amounts of transaction data are generated daily across various platforms, such as Amazon, eBay, and Shopify. This data comes in multiple formats, including CSV, JSON, XML, and others, and often contains inconsistencies such as missing values, incorrect product descriptions, and formatting issues. These challenges make it difficult to process, clean and transform the data efficiently for analytics or machine learning tasks. In order to address these challenges, our project aims to use real-life datasets to train AI-powered agents and generate Python classes to automate the detection of data formats, schema inference, cleaning, transformation, and storage, improving scalability, accuracy, and efficiency in e-commerce data workflows. As a result, the cleaned-up datasets would satisfy future data analysis purposes.  \n\nIn this project, we will use raw datasets crawled from several E-commerce platforms such as Amazon, eBay, and Shopify as training data, which will contain missing values, data of various formats, values that do not make sense, etc. We will leverage Large Language Models (LLMs) including OpenAI's models, Anthropic AI model, and Meta AI offerings to generate code for data transformation, cleaning, and validation processes through advanced prompt engineering techniques. Finally, we will test our model on synthetic data to enhance model performance.\n\nOur approach will explore potential significant improvements in data processing efficiency and accuracy compared to traditional systems. The study will assess the agent’s capacity to manage schema mapping, automate everyday data transformation tasks, and expedite integration setup procedures, ultimately making large-scale e-commerce data management more efficient and effective for future analysis.\n\n## Data Source and Preparation\n\n**Data source** \n\nWe crawled the data from the Amazon E-commerce platform, including the products from many categories, for example, electronics, makeup, furniture, stuffed animals, pet care, and so on. The variables of each entry include: product_categories, product_id, rating_start, Review_counts, Price, Amazon Prime (yes or no), product_url, etc. The crawled data have missing values, integers, dates, and complicated, unstructured text data. The data types include excel format, json format, csv format, and xml format. \n\n**Data Preparation**\n\nTo make the dataset more diverse, we crawled data from different products and saved them in every data type. So, we will have 3 to 4 different datasets in every data type. We also intentionally make the dataset schema different for each data type, so the agent will be familiar with how to deal with a variety of input data during the training step. \n\n## Project Workflow\n\n![](/pics/workflow.png)\n\n\nFrom the above workflow, we can start the AI-enhanced data engineering from very begining.\n\n### Data Preperation\nWe crawl the data from Amazon, and split the data into:\n\n- Raw Training data\n- Raw Test data\n  \nFor the training data, we manipulate the whole cleaning process and saved the cleaned Training data for the Retrieval Argumented Generation Process.\n\n### RAG Process\nAfter we have the cleaning example, we used Faiss to store the dataset schema and cleaning process into vector database.\n\n### Multi-Agents System\n\nWe have 3 agents and each agent will perform its role. \n\n- Agent 1: Retrieval from the database and detect the input test data file, forming the cleaning instruction to the next agent.\n- Agent 2: Follow the first agent's insruction generating the cleaning code.\n- Agent 3: Execute the code that generated from Agent 2, and collect the error information. If error occured, it would return the error message to the second agent to regenerate code until the code work.\n  \n### Evaluation\n\nTo assess the performance of our AI-enhanced data cleaning system, we conducted a comprehensive evaluation focusing on both the quality and consistency of the outputs.  \nWe used two complementary approaches:\n\n- **Quality Evaluation (RAGAS Framework)**:  \n  We measured the faithfulness, relevancy, and factual correctness of the AI-generated cleaning instructions across different file formats.\n\n- **Consistency Evaluation**:  \n  We analyzed the similarity of prompts, generated code, and final cleaned outputs across varied formats (CSV, JSON, XLSX, XML) to verify robustness and stability.\n\nThese evaluations allowed us to assess not only how accurately the system generates responses but also how reliably it maintains consistent behavior across heterogeneous data inputs.  \nThe results highlight both the strengths of our current system and areas for future improvement.\n\n\n","srcMarkdownNoYaml":"\n# AI-Enhanced E-commerce Data Engineering Project Introduction\n\nTeam Members: Yuer Wang, Zining Wang, Jiatong Liu, Xue Qin\n\n## Abstract \n\nAs e-commerce continues to grow rapidly, massive amounts of transaction data are generated daily across various platforms, such as Amazon, eBay, and Shopify. This data comes in multiple formats, including CSV, JSON, XML, and others, and often contains inconsistencies such as missing values, incorrect product descriptions, and formatting issues. These challenges make it difficult to process, clean and transform the data efficiently for analytics or machine learning tasks. In order to address these challenges, our project aims to use real-life datasets to train AI-powered agents and generate Python classes to automate the detection of data formats, schema inference, cleaning, transformation, and storage, improving scalability, accuracy, and efficiency in e-commerce data workflows. As a result, the cleaned-up datasets would satisfy future data analysis purposes.  \n\nIn this project, we will use raw datasets crawled from several E-commerce platforms such as Amazon, eBay, and Shopify as training data, which will contain missing values, data of various formats, values that do not make sense, etc. We will leverage Large Language Models (LLMs) including OpenAI's models, Anthropic AI model, and Meta AI offerings to generate code for data transformation, cleaning, and validation processes through advanced prompt engineering techniques. Finally, we will test our model on synthetic data to enhance model performance.\n\nOur approach will explore potential significant improvements in data processing efficiency and accuracy compared to traditional systems. The study will assess the agent’s capacity to manage schema mapping, automate everyday data transformation tasks, and expedite integration setup procedures, ultimately making large-scale e-commerce data management more efficient and effective for future analysis.\n\n## Data Source and Preparation\n\n**Data source** \n\nWe crawled the data from the Amazon E-commerce platform, including the products from many categories, for example, electronics, makeup, furniture, stuffed animals, pet care, and so on. The variables of each entry include: product_categories, product_id, rating_start, Review_counts, Price, Amazon Prime (yes or no), product_url, etc. The crawled data have missing values, integers, dates, and complicated, unstructured text data. The data types include excel format, json format, csv format, and xml format. \n\n**Data Preparation**\n\nTo make the dataset more diverse, we crawled data from different products and saved them in every data type. So, we will have 3 to 4 different datasets in every data type. We also intentionally make the dataset schema different for each data type, so the agent will be familiar with how to deal with a variety of input data during the training step. \n\n## Project Workflow\n\n![](/pics/workflow.png)\n\n\nFrom the above workflow, we can start the AI-enhanced data engineering from very begining.\n\n### Data Preperation\nWe crawl the data from Amazon, and split the data into:\n\n- Raw Training data\n- Raw Test data\n  \nFor the training data, we manipulate the whole cleaning process and saved the cleaned Training data for the Retrieval Argumented Generation Process.\n\n### RAG Process\nAfter we have the cleaning example, we used Faiss to store the dataset schema and cleaning process into vector database.\n\n### Multi-Agents System\n\nWe have 3 agents and each agent will perform its role. \n\n- Agent 1: Retrieval from the database and detect the input test data file, forming the cleaning instruction to the next agent.\n- Agent 2: Follow the first agent's insruction generating the cleaning code.\n- Agent 3: Execute the code that generated from Agent 2, and collect the error information. If error occured, it would return the error message to the second agent to regenerate code until the code work.\n  \n### Evaluation\n\nTo assess the performance of our AI-enhanced data cleaning system, we conducted a comprehensive evaluation focusing on both the quality and consistency of the outputs.  \nWe used two complementary approaches:\n\n- **Quality Evaluation (RAGAS Framework)**:  \n  We measured the faithfulness, relevancy, and factual correctness of the AI-generated cleaning instructions across different file formats.\n\n- **Consistency Evaluation**:  \n  We analyzed the similarity of prompts, generated code, and final cleaned outputs across varied formats (CSV, JSON, XLSX, XML) to verify robustness and stability.\n\nThese evaluations allowed us to assess not only how accurately the system generates responses but also how reliably it maintains consistent behavior across heterogeneous data inputs.  \nThe results highlight both the strengths of our current system and areas for future improvement.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"introduction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","theme":"morph","title":"Introduction"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}