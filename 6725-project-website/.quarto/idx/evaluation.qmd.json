{"title":"Overview","markdown":{"headingText":"Overview","containsRefs":false,"markdown":"The evaluation process tests two main aspects: quality assessment (how effectively it cleans and processes data) and format consistency (how well the system handles different file formats). The system uses both the RAGAS framework for evaluation and custom metrics.\n\n\n## Quality Evaluation with RAGAS\n\nIn this evaluation, we assessed how well the Agent generated cleaning prompt are aligned with our best practices for different file formats (JSON, XLSX, XML, CSV).\n\nThe quality assessment uses the RAGAS framework to evaluate three key dimensions: \n - Faithfulness: Measures how much the AI stuck to the given reference materials.\n - Answer Relevancy: Measures how well the AI answered the question asked.\n - Answer Correctness: Measures whether the AI's response was factually correct based on the best practices provided.\n\n### Methodology\nWe first constructed a dataset pairing each AI-generated response with:\n - A user input question (e.g., \"How to clean CSV data?\"),\n - Retrieved context (reference best practices for the format),\n - A ground-truth reference answer.\n\nUsing RAGAS, we scored each response individually on faithfulness, relevancy, and correctness.\n\nInstead of immediately averaging the results, we first recorded per-example scores for each prompt, providing detailed insights into how the AI performed across different file formats.\n\nFinally, we computed an overall average score for each metric to summarize model performance.\n\n### Results\n| Question                | Faithfulness | Answer Relevancy | Answer Correctness |\n|--------------------------|--------------|------------------|--------------------|\n| How to clean JSON data   | 0.39         | 0.62             | 0.46               |\n| How to clean XLSX data   | 0.54         | 0.74             | 0.72               |\n| How to clean XML data    | 0.64         | 0.74             | 0.37               |\n| How to clean CSV data    | 0.62         | 0.77             | 0.68               |\n| **Summary**              | **0.55**     | **0.72**         | **0.56**           |\n\n### Key Findings\nRelevancy was the strongest:\nAI mostly stayed on topic and answered the right questions.\n\nFaithfulness and correctness were moderate:\nThe AI sometimes added extra information or missed details from the references, especially for more complex formats like JSON and XML.\n\nCSV and XLSX responses were the best:\nCleaning instructions for CSV and Excel files were generally more faithful and factually correct compared to JSON and XML.\n\n## Format Consistency Evalution\nIn addition to quality evaluation, we assessed the consistency of the AI-generated outputs across different data formats.\nThe goal was to ensure that the AI not only answered correctly, but did so consistently even when facing different types of input files.\n\n### Metrics\nWe measured consistency along three axes:\n - Prompt Similarity:\n    How similar the AI-generated cleaning instructions were across different formats.\n - Code Similarity:\n    How similar the cleaning code outputs were across different formats.\n - Output Similarity:\n    How similar the final cleaned datasets were after applying the AI-generated cleaning instructions.\n\n### Methodology\n### Data for evaluation:\nSynthetic dataset: \n\tDATA 1 (evaluation_data.csv):\n      \t - Schema Consistency\n      \t - Mimic real-world datatype and formatting\n      \t - Controlled Diversity: diverse set of categories allow the agent see different kinds of inputs\n      \t - Edge-Case Readiness (Basic): missing values, 0 reviews, no prime membership, high and moderately low scores for rating. \n\tDATA 2 (edge cases.csv):\n\t\t - Star Ratings: Includes very low (1.0) and out-of-bound (5.5) star ratings, and some missing values (NaN).\n\t\t - ReviewsCount: Includes extreme values like 0, 1, 5, 10,000, 50,000 — to test how well your system handles abnormal popularity.\n\t\t - Price: Includes ultra-low ($0.01), ultra-high ($19,999.99), missing prices (empty string), and normal prices.\n\t\t - Prime: Only 50% are Prime eligible (more variability).\n\t\t - Order/Page Number: Randomized across a wider range (order up to 20, page up to 10).\n\t\t - Timestamps: Random timestamps across a full day instead of just the first hour.\n\nFor each dataset and format (CSV, JSON, XLSX, XML):\n - Prompt Similarity:\n    We tokenized the cleaning prompts and computed the Jaccard similarity based on overlapping words.\n - Code Similarity:\n    We cleaned the generated code (removing comments and docstrings), normalized whitespace, and computed token-based Jaccard similarity.\n - Output Similarity:\n    We compared the final cleaned datasets by:\n     - Checking schema similarity (matching columns and types),\n     - Hashing row contents and comparing the resulting hash sets,\n     - Computing a weighted score: 40% schema match + 60% content match.\n\n### Results\n| File                  | Formats Tested        | Prompt Similarity | Code Similarity | Output Similarity |\n|-----------------------|------------------------|-------------------|-----------------|-------------------|\n| edge_cases.csv         | csv, json, xlsx, xml    | 0.54              | 0.61            | 1.0               |\n| evaluation_data.csv    | csv, json, xlsx, xml    | 0.60              | 0.65            | 1.0               |\n\n\n### Findings\nPrompt Similarity and Code Similarity were moderately high (~0.54–0.65), reflecting minor adjustments made for different file types.\n\nOutput Similarity achieved a perfect score of 1.0, demonstrating that despite minor wording or code differences, the AI consistently produced identical final cleaned outputs across formats.\n\nThis consistency highlights the robustness of the AI data cleaning agent, ensuring reliability even when working with heterogeneous and messy input data.","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"evaluation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.549","theme":"morph"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}