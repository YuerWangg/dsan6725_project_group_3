[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Team Members: Yuer Wang, Zining Wang, Jiatong Liu, Xue Qin\n\n\nAs e-commerce continues to grow rapidly, massive amounts of transaction data are generated daily across various platforms, such as Amazon, eBay, and Shopify. This data comes in multiple formats, including CSV, JSON, XML, and others, and often contains inconsistencies such as missing values, incorrect product descriptions, and formatting issues. These challenges make it difficult to process, clean and transform the data efficiently for analytics or machine learning tasks. In order to address these challenges, our project aims to use real-life datasets to train AI-powered agents and generate Python classes to automate the detection of data formats, schema inference, cleaning, transformation, and storage, improving scalability, accuracy, and efficiency in e-commerce data workflows. As a result, the cleaned-up datasets would satisfy future data analysis purposes.\nIn this project, we will use raw datasets crawled from several E-commerce platforms such as Amazon, eBay, and Shopify as training data, which will contain missing values, data of various formats, values that do not make sense, etc. We will leverage Large Language Models (LLMs) including OpenAI‚Äôs models, Anthropic AI model, and Meta AI offerings to generate code for data transformation, cleaning, and validation processes through advanced prompt engineering techniques. Finally, we will test our model on synthetic data to enhance model performance.\nOur approach will explore potential significant improvements in data processing efficiency and accuracy compared to traditional systems. The study will assess the agent‚Äôs capacity to manage schema mapping, automate everyday data transformation tasks, and expedite integration setup procedures, ultimately making large-scale e-commerce data management more efficient and effective for future analysis.\n\n\n\nData source\nWe crawled the data from the Amazon E-commerce platform, including the products from many categories, for example, electronics, makeup, furniture, stuffed animals, pet care, and so on. The variables of each entry include: product_categories, product_id, rating_start, Review_counts, Price, Amazon Prime (yes or no), product_url, etc. The crawled data have missing values, integers, dates, and complicated, unstructured text data. The data types include excel format, json format, csv format, and xml format.\nData Preparation\nTo make the dataset more diverse, we crawled data from different products and saved them in every data type. So, we will have 3 to 4 different datasets in every data type. We also intentionally make the dataset schema different for each data type, so the agent will be familiar with how to deal with a variety of input data during the training step.\n\n\n\n\nFrom the above workflow, we can start the AI-enhanced data engineering from very begining.\n\n\nWe crawl the data from Amazon, and split the data into:\n\nRaw Training data\nRaw Test data\n\nFor the training data, we manipulate the whole cleaning process and saved the cleaned Training data for the Retrieval Argumented Generation Process.\n\n\n\nAfter we have the cleaning example, we used Faiss to store the dataset schema and cleaning process into vector database.\n\n\n\nWe have 3 agents and each agent will perform its role.\n\nAgent 1: Retrieval from the database and detect the input test data file, forming the cleaning instruction to the next agent.\nAgent 2: Follow the first agent‚Äôs insruction generating the cleaning code.\nAgent 3: Execute the code that generated from Agent 2, and collect the error information. If error occured, it would return the error message to the second agent to regenerate code until the code work.\n\n\n\n\nTo assess the performance of our AI-enhanced data cleaning system, we conducted a comprehensive evaluation focusing on both the quality and consistency of the outputs.\nWe used two complementary approaches:\n\nQuality Evaluation (RAGAS Framework):\nWe measured the faithfulness, relevancy, and factual correctness of the AI-generated cleaning instructions across different file formats.\nConsistency Evaluation:\nWe analyzed the similarity of prompts, generated code, and final cleaned outputs across varied formats (CSV, JSON, XLSX, XML) to verify robustness and stability.\n\nThese evaluations allowed us to assess not only how accurately the system generates responses but also how reliably it maintains consistent behavior across heterogeneous data inputs.\nThe results highlight both the strengths of our current system and areas for future improvement."
  },
  {
    "objectID": "introduction.html#abstract",
    "href": "introduction.html#abstract",
    "title": "Introduction",
    "section": "",
    "text": "As e-commerce continues to grow rapidly, massive amounts of transaction data are generated daily across various platforms, such as Amazon, eBay, and Shopify. This data comes in multiple formats, including CSV, JSON, XML, and others, and often contains inconsistencies such as missing values, incorrect product descriptions, and formatting issues. These challenges make it difficult to process, clean and transform the data efficiently for analytics or machine learning tasks. In order to address these challenges, our project aims to use real-life datasets to train AI-powered agents and generate Python classes to automate the detection of data formats, schema inference, cleaning, transformation, and storage, improving scalability, accuracy, and efficiency in e-commerce data workflows. As a result, the cleaned-up datasets would satisfy future data analysis purposes.\nIn this project, we will use raw datasets crawled from several E-commerce platforms such as Amazon, eBay, and Shopify as training data, which will contain missing values, data of various formats, values that do not make sense, etc. We will leverage Large Language Models (LLMs) including OpenAI‚Äôs models, Anthropic AI model, and Meta AI offerings to generate code for data transformation, cleaning, and validation processes through advanced prompt engineering techniques. Finally, we will test our model on synthetic data to enhance model performance.\nOur approach will explore potential significant improvements in data processing efficiency and accuracy compared to traditional systems. The study will assess the agent‚Äôs capacity to manage schema mapping, automate everyday data transformation tasks, and expedite integration setup procedures, ultimately making large-scale e-commerce data management more efficient and effective for future analysis."
  },
  {
    "objectID": "introduction.html#data-source-and-preparation",
    "href": "introduction.html#data-source-and-preparation",
    "title": "Introduction",
    "section": "",
    "text": "Data source\nWe crawled the data from the Amazon E-commerce platform, including the products from many categories, for example, electronics, makeup, furniture, stuffed animals, pet care, and so on. The variables of each entry include: product_categories, product_id, rating_start, Review_counts, Price, Amazon Prime (yes or no), product_url, etc. The crawled data have missing values, integers, dates, and complicated, unstructured text data. The data types include excel format, json format, csv format, and xml format.\nData Preparation\nTo make the dataset more diverse, we crawled data from different products and saved them in every data type. So, we will have 3 to 4 different datasets in every data type. We also intentionally make the dataset schema different for each data type, so the agent will be familiar with how to deal with a variety of input data during the training step."
  },
  {
    "objectID": "introduction.html#project-workflow",
    "href": "introduction.html#project-workflow",
    "title": "Introduction",
    "section": "",
    "text": "From the above workflow, we can start the AI-enhanced data engineering from very begining.\n\n\nWe crawl the data from Amazon, and split the data into:\n\nRaw Training data\nRaw Test data\n\nFor the training data, we manipulate the whole cleaning process and saved the cleaned Training data for the Retrieval Argumented Generation Process.\n\n\n\nAfter we have the cleaning example, we used Faiss to store the dataset schema and cleaning process into vector database.\n\n\n\nWe have 3 agents and each agent will perform its role.\n\nAgent 1: Retrieval from the database and detect the input test data file, forming the cleaning instruction to the next agent.\nAgent 2: Follow the first agent‚Äôs insruction generating the cleaning code.\nAgent 3: Execute the code that generated from Agent 2, and collect the error information. If error occured, it would return the error message to the second agent to regenerate code until the code work.\n\n\n\n\nTo assess the performance of our AI-enhanced data cleaning system, we conducted a comprehensive evaluation focusing on both the quality and consistency of the outputs.\nWe used two complementary approaches:\n\nQuality Evaluation (RAGAS Framework):\nWe measured the faithfulness, relevancy, and factual correctness of the AI-generated cleaning instructions across different file formats.\nConsistency Evaluation:\nWe analyzed the similarity of prompts, generated code, and final cleaned outputs across varied formats (CSV, JSON, XLSX, XML) to verify robustness and stability.\n\nThese evaluations allowed us to assess not only how accurately the system generates responses but also how reliably it maintains consistent behavior across heterogeneous data inputs.\nThe results highlight both the strengths of our current system and areas for future improvement."
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "AI-Enhanced E-Commerce Data Engineering",
    "section": "",
    "text": "The evaluation process tests two main aspects: quality assessment (how effectively it cleans and processes data) and format consistency (how well the system handles different file formats). The system uses both the RAGAS framework for evaluation and custom metrics."
  },
  {
    "objectID": "evaluation.html#overview",
    "href": "evaluation.html#overview",
    "title": "AI-Enhanced E-Commerce Data Engineering",
    "section": "",
    "text": "The evaluation process tests two main aspects: quality assessment (how effectively it cleans and processes data) and format consistency (how well the system handles different file formats). The system uses both the RAGAS framework for evaluation and custom metrics."
  },
  {
    "objectID": "evaluation.html#quality-evaluation-with-ragas",
    "href": "evaluation.html#quality-evaluation-with-ragas",
    "title": "AI-Enhanced E-Commerce Data Engineering",
    "section": "Quality Evaluation with RAGAS",
    "text": "Quality Evaluation with RAGAS\nIn this evaluation, we assessed how well the Agent generated cleaning prompt are aligned with our best practices for different file formats (JSON, XLSX, XML, CSV).\nThe quality assessment uses the RAGAS framework to evaluate three key dimensions: - Faithfulness: Measures how much the AI stuck to the given reference materials. - Answer Relevancy: Measures how well the AI answered the question asked. - Answer Correctness: Measures whether the AI‚Äôs response was factually correct based on the best practices provided.\n\nMethodology\nWe first constructed a dataset pairing each AI-generated response with: - A user input question (e.g., ‚ÄúHow to clean CSV data?‚Äù), - Retrieved context (reference best practices for the format), - A ground-truth reference answer.\nUsing RAGAS, we scored each response individually on faithfulness, relevancy, and correctness.\nInstead of immediately averaging the results, we first recorded per-example scores for each prompt, providing detailed insights into how the AI performed across different file formats.\nFinally, we computed an overall average score for each metric to summarize model performance.\n\n\nResults\n\n\n\n\n\n\n\n\n\nQuestion\nFaithfulness\nAnswer Relevancy\nAnswer Correctness\n\n\n\n\nHow to clean JSON data\n0.39\n0.62\n0.46\n\n\nHow to clean XLSX data\n0.54\n0.74\n0.72\n\n\nHow to clean XML data\n0.64\n0.74\n0.37\n\n\nHow to clean CSV data\n0.62\n0.77\n0.68\n\n\nSummary\n0.55\n0.72\n0.56\n\n\n\n\n\nKey Findings\nRelevancy was the strongest: AI mostly stayed on topic and answered the right questions.\nFaithfulness and correctness were moderate: The AI sometimes added extra information or missed details from the references, especially for more complex formats like JSON and XML.\nCSV and XLSX responses were the best: Cleaning instructions for CSV and Excel files were generally more faithful and factually correct compared to JSON and XML."
  },
  {
    "objectID": "evaluation.html#format-consistency-evalution",
    "href": "evaluation.html#format-consistency-evalution",
    "title": "AI-Enhanced E-Commerce Data Engineering",
    "section": "Format Consistency Evalution",
    "text": "Format Consistency Evalution\nIn addition to quality evaluation, we assessed the consistency of the AI-generated outputs across different data formats. The goal was to ensure that the AI not only answered correctly, but did so consistently even when facing different types of input files.\n\nMetrics\nWe measured consistency along three axes: - Prompt Similarity: How similar the AI-generated cleaning instructions were across different formats. - Code Similarity: How similar the cleaning code outputs were across different formats. - Output Similarity: How similar the final cleaned datasets were after applying the AI-generated cleaning instructions.\n\n\nMethodology\n\n\nData for evaluation:\nSynthetic dataset: DATA 1 (evaluation_data.csv): - Schema Consistency - Mimic real-world datatype and formatting - Controlled Diversity: diverse set of categories allow the agent see different kinds of inputs - Edge-Case Readiness (Basic): missing values, 0 reviews, no prime membership, high and moderately low scores for rating. DATA 2 (edge cases.csv): - Star Ratings: Includes very low (1.0) and out-of-bound (5.5) star ratings, and some missing values (NaN). - ReviewsCount: Includes extreme values like 0, 1, 5, 10,000, 50,000 ‚Äî to test how well your system handles abnormal popularity. - Price: Includes ultra-low ($0.01), ultra-high ($19,999.99), missing prices (empty string), and normal prices. - Prime: Only 50% are Prime eligible (more variability). - Order/Page Number: Randomized across a wider range (order up to 20, page up to 10). - Timestamps: Random timestamps across a full day instead of just the first hour.\nFor each dataset and format (CSV, JSON, XLSX, XML): - Prompt Similarity: We tokenized the cleaning prompts and computed the Jaccard similarity based on overlapping words. - Code Similarity: We cleaned the generated code (removing comments and docstrings), normalized whitespace, and computed token-based Jaccard similarity. - Output Similarity: We compared the final cleaned datasets by: - Checking schema similarity (matching columns and types), - Hashing row contents and comparing the resulting hash sets, - Computing a weighted score: 40% schema match + 60% content match.\n\n\nResults\n\n\n\n\n\n\n\n\n\n\nFile\nFormats Tested\nPrompt Similarity\nCode Similarity\nOutput Similarity\n\n\n\n\nedge_cases.csv\ncsv, json, xlsx, xml\n0.54\n0.61\n1.0\n\n\nevaluation_data.csv\ncsv, json, xlsx, xml\n0.60\n0.65\n1.0\n\n\n\n\n\nFindings\nPrompt Similarity and Code Similarity were moderately high (~0.54‚Äì0.65), reflecting minor adjustments made for different file types.\nOutput Similarity achieved a perfect score of 1.0, demonstrating that despite minor wording or code differences, the AI consistently produced identical final cleaned outputs across formats.\nThis consistency highlights the robustness of the AI data cleaning agent, ensuring reliability even when working with heterogeneous and messy input data."
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Demo",
    "section": "",
    "text": "Demo\nThis is a short demo about how to use this project"
  },
  {
    "objectID": "rag.html",
    "href": "rag.html",
    "title": "Retrival Argumented Generation",
    "section": "",
    "text": "This RAG (Retrieval-Augmented Generation) system is designed to process and analyze structured and semi-structured data from various file formats, including JSON, CSV, XML, Parquet, and Excel, and saved in the faiss index in parquet form. It ingests data, cleans and preprocesses it (e.g., standardizing column names, converting text-based numbers like ‚Äú1.5K+‚Äù to numeric values), and splits the content into manageable chunks. These chunks are then embedded into a vector space using the HuggingFace BGE model, enabling efficient similarity-based retrieval. The system stores these embeddings in FAISS indexes for quick lookup and supports hybrid retrieval methods like BM25 and ensemble techniques to improve accuracy.\n\n\n\nThe RAG system also includes specialized indexes for transformation rules, user prompts, and protential data schemas, allowing it to provide context-aware solutions for common data processing tasks (e.g., handling missing values, calculating revenue, or cleaning product descriptions), specific to each file type. Below are some sample cleaning code to that are commonly used among all data file types, that are stored in the faiss indexes.\n[\n\"Remove null values: df.dropna()\",\n\n\"Fill null values with a specific value: df.fillna(value)\",\n\n\"Standardize column names: df.columns = df.columns.str.lower().str.replace(' ', '_')\",\n\n\"Remove duplicate rows: df.drop_duplicates()\",\n\n\"Convert data types: df['column'] = df['column'].astype(type)\",\n\n\"Filter rows based on a condition: df[df['column'] &gt; value]\",\n\n\"Rename columns: df.rename(columns={'old_name': 'new_name'})\",\n\n\"Drop columns: df.drop(columns=['column_name'])\",\n\n\"Sort dataframe: df.sort_values(by='column_name')\",\n]\n\n\n\nBy combining multiple retrievers for each faiss index into a unified MultiVectorRetriever, it ensures comprehensive results for queries. The system is integrated with LangChain to facilitate question-answering workflows, where retrieved documents augment the responses of a language model.\n\n\n\nOverall, this RAG pipeline enhances data accessibility and analysis, making it particularly useful for e-commerce, inventory management, and financial calculations.\nHowever, the limitations of RAG and it‚Äôs connection to the code-generating agent exist. Based on the results of several tryouts, the agent still hallucinates; the RetrievalQA chain (via LangChain) helps mitigate this, but if the retriever returns weak matches, the LLM might still generate incorrect or unsupported responses.\nThe transformation_rules and user_prompts indexes are pre-defined. If a user asks a question requiring dynamic or domain-specific logic (e.g., custom aggregations), the agent might generate arbitrary code instead of following documented rules."
  },
  {
    "objectID": "rag.html#knowledge-base-content",
    "href": "rag.html#knowledge-base-content",
    "title": "Retrival Argumented Generation",
    "section": "",
    "text": "This RAG (Retrieval-Augmented Generation) system is designed to process and analyze structured and semi-structured data from various file formats, including JSON, CSV, XML, Parquet, and Excel, and saved in the faiss index in parquet form. It ingests data, cleans and preprocesses it (e.g., standardizing column names, converting text-based numbers like ‚Äú1.5K+‚Äù to numeric values), and splits the content into manageable chunks. These chunks are then embedded into a vector space using the HuggingFace BGE model, enabling efficient similarity-based retrieval. The system stores these embeddings in FAISS indexes for quick lookup and supports hybrid retrieval methods like BM25 and ensemble techniques to improve accuracy."
  },
  {
    "objectID": "rag.html#retrieval-augmented-generation",
    "href": "rag.html#retrieval-augmented-generation",
    "title": "Retrival Argumented Generation",
    "section": "",
    "text": "The RAG system also includes specialized indexes for transformation rules, user prompts, and protential data schemas, allowing it to provide context-aware solutions for common data processing tasks (e.g., handling missing values, calculating revenue, or cleaning product descriptions), specific to each file type. Below are some sample cleaning code to that are commonly used among all data file types, that are stored in the faiss indexes.\n[\n\"Remove null values: df.dropna()\",\n\n\"Fill null values with a specific value: df.fillna(value)\",\n\n\"Standardize column names: df.columns = df.columns.str.lower().str.replace(' ', '_')\",\n\n\"Remove duplicate rows: df.drop_duplicates()\",\n\n\"Convert data types: df['column'] = df['column'].astype(type)\",\n\n\"Filter rows based on a condition: df[df['column'] &gt; value]\",\n\n\"Rename columns: df.rename(columns={'old_name': 'new_name'})\",\n\n\"Drop columns: df.drop(columns=['column_name'])\",\n\n\"Sort dataframe: df.sort_values(by='column_name')\",\n]"
  },
  {
    "objectID": "rag.html#multivectorretriever",
    "href": "rag.html#multivectorretriever",
    "title": "Retrival Argumented Generation",
    "section": "",
    "text": "By combining multiple retrievers for each faiss index into a unified MultiVectorRetriever, it ensures comprehensive results for queries. The system is integrated with LangChain to facilitate question-answering workflows, where retrieved documents augment the responses of a language model."
  },
  {
    "objectID": "rag.html#summary-and-limitation",
    "href": "rag.html#summary-and-limitation",
    "title": "Retrival Argumented Generation",
    "section": "",
    "text": "Overall, this RAG pipeline enhances data accessibility and analysis, making it particularly useful for e-commerce, inventory management, and financial calculations.\nHowever, the limitations of RAG and it‚Äôs connection to the code-generating agent exist. Based on the results of several tryouts, the agent still hallucinates; the RetrievalQA chain (via LangChain) helps mitigate this, but if the retriever returns weak matches, the LLM might still generate incorrect or unsupported responses.\nThe transformation_rules and user_prompts indexes are pre-defined. If a user asks a question requiring dynamic or domain-specific logic (e.g., custom aggregations), the agent might generate arbitrary code instead of following documented rules."
  },
  {
    "objectID": "agent.html",
    "href": "agent.html",
    "title": "Multi-Agent System",
    "section": "",
    "text": "Multi-Agent Systems (MAS) are systems composed of multiple interacting agents, where each agent is an autonomous entity capable of perceiving its environment, making decisions, and taking actions independently or in collaboration with other agents.These agents can cooperate, compete, negotiate, or coordinate to achieve individual or collective goals 1.\nIn this project, we utilized cooperative multi-agent system, and each agent will work cooperatively, and indepently focus on its assigned task. 2 Three agents will be implemented in order to clean up the e-commerce datasets: retrieval and prompt generation agent, code generation agent, and code execution agent. We will use the OpenAI API 3 , gpt-4o model, for all the processes.\n\n\n\nThe retrieval agent is responsible for format detection, schema extraction, contextual retrieval via FAISS and returns a structured context prompt. When a dataset is fed to the agent, it will identify whether it is CSV, JSON, XML, or XLSX using filename suffixes. From here, the file will be parsed to extract structural metadata like column names, types, and sample rows. The agent will query the FAISS vector database for sample data schemas, clean-up rules, and prompt examples for transformation. Eventually a prompt will be generated with matched schema, transformation rules, and examples to be consumed by the generation agent. This framework will be implemented through LangChain Tool abstraction.\n\n\n\nThe code generation agent is responsible for code generation. The agent takes the prompt from the first agent and generates python code for cleaning and transforming the input data.\n\n\n\nThis agent runs the generated code from the second agent to clean the raw data and save the cleaned dataset in the file path. If an error occurs, the error message will be sent back to the second agent to regenerate code, and this agent will run the newly generated code again until it saves the cleaned dataset successfully.\n\n\n\n\n\n‚úÖ Cleaning instructions generated successfully!\n================================================================================ CLEANING INSTRUCTIONS: To clean and preprocess the dataset makeup.csv, follow these detailed instructions:\n\nHandling Missing Values\n\n\nPrime Column: There is one missing value in the Prime column. Since this column indicates whether a product is available with Amazon Prime, you can fill the missing value with ‚ÄúNot Available‚Äù or ‚ÄúNo‚Äù if the context allows, assuming non-Prime availability.\n\n\nDetecting and Treating Outliers\n\n\nStar Ratings: The star column has a minimum value of 2.8, which might be an outlier given the mean of 4.26. Review these entries to ensure they are valid. If they are valid, retain them; otherwise, consider removing or flagging them for further review.\nOrder and Page Number: These columns seem to have reasonable ranges given the context (order of appearance and page number), so no immediate action is needed unless business logic dictates otherwise.\n\n\nData Type Issues and Conversions\n\n\nReviewsCount: Convert this column from object to int64. First, remove commas from the numbers and then convert.\nPrice: Convert this column from object to float64. Remove the dollar sign $ before conversion.\nPrime: Convert this column to a boolean type where ‚ÄúAmazon Prime‚Äù is True and any other value or missing is False.\n\n\nFormat Standardization\n\n\nÂΩìÂâçÊó∂Èó¥: This column is already in a datetime format, but ensure it is recognized as such by converting it to datetime64[ns] if necessary.\nPrice: Ensure all prices are in a consistent currency format, assuming USD based on the dollar sign.\n\n\nCategorical Encoding\n\n\nKey Words: Convert this column to a categorical type. If needed for modeling, use one-hot encoding.\nPrime: As mentioned, convert to boolean for easier analysis.\n\n\nText Fields Cleaning and Normalization\n\n\nTitle: Normalize text by converting to lowercase to ensure consistency in text analysis.\nTitle URL: This column seems to be a URL and might not need cleaning unless used for specific analysis.\n\n\nIdentifying Potential Duplicates\n\n\nDuplicate Rows: The dataset reports zero duplicate rows. However, ensure there are no logical duplicates by checking for identical asin values, as this should be unique for each product.\n\n\nRenaming Columns for Clarity\n\n\nÂΩìÂâçÊó∂Èó¥: Rename to current_time for consistency and clarity.\nTitle URL: Rename to product_url for clarity.\n\n\nDerived Features\n\n\nReview to Price Ratio: Create a new feature by dividing ReviewsCount by price to understand the review density relative to price.\nStar to Price Ratio: Create a new feature by dividing star by price to assess value for money based on ratings.\nIs Prime: As a boolean feature, this can be used directly in models to understand the impact of Prime availability.\n\nAdditional StepsÔºö\n\nData Validation: After cleaning, validate the data types and check for any remaining inconsistencies.\nDocumentation: Document all changes made to the dataset for reproducibility and future reference.\n\nBy following these steps, you will ensure the dataset is clean, consistent, and ready for analysis or modeling.\n================================================================================\n\n\n\n‚úÖ Cleaning code generated successfully!\nüíæ Cleaning code saved to /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\n================================================================================\n\n\n\nExecuting cleaning code‚Ä¶ Warning: No main() function found in generated code. Trying subprocess execution‚Ä¶ Error executing cleaning code: STDOUT: STDERR: /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method. The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\nFor example, when doing ‚Äòdf[col].method(value, inplace=True)‚Äô, try using ‚Äòdf.method({col: value}, inplace=True)‚Äô or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\ndf[‚ÄòPrime‚Äô].fillna(‚ÄòNo‚Äô, inplace=True)\n\nTraceback (most recent call last):\nFile \"/Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\", line 73, in &lt;module&gt;\n    clean_makeup_data(filepath)\nFile \"/Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\", line 21, in clean_makeup_data\n    df['ReviewsCount'] = df['ReviewsCount'].str.replace(',', '').astype('int64')\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 430, in astype\n    return self.apply(\n        ^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: cannot convert float NaN to integer\n\n‚ùå Execution failed. Error: Error executing cleaning code:\nSTDOUT:\nSTDERR: /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py:13:\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\nFor example, when doing ‚Äòdf[col].method(value, inplace=True)‚Äô, try using ‚Äòdf.method({col: value}, inplace=True)‚Äô or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n================================================================================\n\n\n\nüîÑ Attempting code regeneration (1/3)‚Ä¶\n\n\n‚úÖ Cleaning code regenerated successfully!\nüíæ Cleaning code saved to /Users/sherryqin/Desktop/agent/clean_makeup_attempt2.py\n\n\n\nExecuting cleaning code‚Ä¶ Warning: No main() function found in generated code. Trying subprocess execution‚Ä¶\n================================================================================\n‚úÖ Cleaning completed successfully! Cleaned data saved to cleaned_makeup.csv\nüéâ The complete data cleaning pipeline finished successfully! Original data: makeup.csv\nCleaned data: cleaned_makeup.csv\nDATA SUMMARY:\nRows: 1006\nColumns: 13\nMissing values: 145"
  },
  {
    "objectID": "agent.html#retrieval-and-prompt-generation-agent",
    "href": "agent.html#retrieval-and-prompt-generation-agent",
    "title": "Multi-Agent System",
    "section": "",
    "text": "The retrieval agent is responsible for format detection, schema extraction, contextual retrieval via FAISS and returns a structured context prompt. When a dataset is fed to the agent, it will identify whether it is CSV, JSON, XML, or XLSX using filename suffixes. From here, the file will be parsed to extract structural metadata like column names, types, and sample rows. The agent will query the FAISS vector database for sample data schemas, clean-up rules, and prompt examples for transformation. Eventually a prompt will be generated with matched schema, transformation rules, and examples to be consumed by the generation agent. This framework will be implemented through LangChain Tool abstraction."
  },
  {
    "objectID": "agent.html#code-generation-agent",
    "href": "agent.html#code-generation-agent",
    "title": "Multi-Agent System",
    "section": "",
    "text": "The code generation agent is responsible for code generation. The agent takes the prompt from the first agent and generates python code for cleaning and transforming the input data."
  },
  {
    "objectID": "agent.html#code-execution-agent",
    "href": "agent.html#code-execution-agent",
    "title": "Multi-Agent System",
    "section": "",
    "text": "This agent runs the generated code from the second agent to clean the raw data and save the cleaned dataset in the file path. If an error occurs, the error message will be sent back to the second agent to regenerate code, and this agent will run the newly generated code again until it saves the cleaned dataset successfully."
  },
  {
    "objectID": "agent.html#example-usage",
    "href": "agent.html#example-usage",
    "title": "Multi-Agent System",
    "section": "",
    "text": "‚úÖ Cleaning instructions generated successfully!\n================================================================================ CLEANING INSTRUCTIONS: To clean and preprocess the dataset makeup.csv, follow these detailed instructions:\n\nHandling Missing Values\n\n\nPrime Column: There is one missing value in the Prime column. Since this column indicates whether a product is available with Amazon Prime, you can fill the missing value with ‚ÄúNot Available‚Äù or ‚ÄúNo‚Äù if the context allows, assuming non-Prime availability.\n\n\nDetecting and Treating Outliers\n\n\nStar Ratings: The star column has a minimum value of 2.8, which might be an outlier given the mean of 4.26. Review these entries to ensure they are valid. If they are valid, retain them; otherwise, consider removing or flagging them for further review.\nOrder and Page Number: These columns seem to have reasonable ranges given the context (order of appearance and page number), so no immediate action is needed unless business logic dictates otherwise.\n\n\nData Type Issues and Conversions\n\n\nReviewsCount: Convert this column from object to int64. First, remove commas from the numbers and then convert.\nPrice: Convert this column from object to float64. Remove the dollar sign $ before conversion.\nPrime: Convert this column to a boolean type where ‚ÄúAmazon Prime‚Äù is True and any other value or missing is False.\n\n\nFormat Standardization\n\n\nÂΩìÂâçÊó∂Èó¥: This column is already in a datetime format, but ensure it is recognized as such by converting it to datetime64[ns] if necessary.\nPrice: Ensure all prices are in a consistent currency format, assuming USD based on the dollar sign.\n\n\nCategorical Encoding\n\n\nKey Words: Convert this column to a categorical type. If needed for modeling, use one-hot encoding.\nPrime: As mentioned, convert to boolean for easier analysis.\n\n\nText Fields Cleaning and Normalization\n\n\nTitle: Normalize text by converting to lowercase to ensure consistency in text analysis.\nTitle URL: This column seems to be a URL and might not need cleaning unless used for specific analysis.\n\n\nIdentifying Potential Duplicates\n\n\nDuplicate Rows: The dataset reports zero duplicate rows. However, ensure there are no logical duplicates by checking for identical asin values, as this should be unique for each product.\n\n\nRenaming Columns for Clarity\n\n\nÂΩìÂâçÊó∂Èó¥: Rename to current_time for consistency and clarity.\nTitle URL: Rename to product_url for clarity.\n\n\nDerived Features\n\n\nReview to Price Ratio: Create a new feature by dividing ReviewsCount by price to understand the review density relative to price.\nStar to Price Ratio: Create a new feature by dividing star by price to assess value for money based on ratings.\nIs Prime: As a boolean feature, this can be used directly in models to understand the impact of Prime availability.\n\nAdditional StepsÔºö\n\nData Validation: After cleaning, validate the data types and check for any remaining inconsistencies.\nDocumentation: Document all changes made to the dataset for reproducibility and future reference.\n\nBy following these steps, you will ensure the dataset is clean, consistent, and ready for analysis or modeling.\n================================================================================\n\n\n\n‚úÖ Cleaning code generated successfully!\nüíæ Cleaning code saved to /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\n================================================================================\n\n\n\nExecuting cleaning code‚Ä¶ Warning: No main() function found in generated code. Trying subprocess execution‚Ä¶ Error executing cleaning code: STDOUT: STDERR: /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method. The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\nFor example, when doing ‚Äòdf[col].method(value, inplace=True)‚Äô, try using ‚Äòdf.method({col: value}, inplace=True)‚Äô or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\ndf[‚ÄòPrime‚Äô].fillna(‚ÄòNo‚Äô, inplace=True)\n\nTraceback (most recent call last):\nFile \"/Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\", line 73, in &lt;module&gt;\n    clean_makeup_data(filepath)\nFile \"/Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py\", line 21, in clean_makeup_data\n    df['ReviewsCount'] = df['ReviewsCount'].str.replace(',', '').astype('int64')\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 6643, in astype\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 430, in astype\n    return self.apply(\n        ^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 363, in apply\n    applied = getattr(b, f)(**kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 758, in astype\n    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\n    new_values = astype_array(values, dtype, copy=copy)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\n    values = _astype_nansafe(values, dtype, copy=copy)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/sherryqin/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py\", line 133, in _astype_nansafe\n    return arr.astype(dtype, copy=True)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: cannot convert float NaN to integer\n\n‚ùå Execution failed. Error: Error executing cleaning code:\nSTDOUT:\nSTDERR: /Users/sherryqin/Desktop/agent/clean_makeup_attempt1.py:13:\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\nFor example, when doing ‚Äòdf[col].method(value, inplace=True)‚Äô, try using ‚Äòdf.method({col: value}, inplace=True)‚Äô or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n================================================================================\n\n\n\nüîÑ Attempting code regeneration (1/3)‚Ä¶\n\n\n‚úÖ Cleaning code regenerated successfully!\nüíæ Cleaning code saved to /Users/sherryqin/Desktop/agent/clean_makeup_attempt2.py\n\n\n\nExecuting cleaning code‚Ä¶ Warning: No main() function found in generated code. Trying subprocess execution‚Ä¶\n================================================================================\n‚úÖ Cleaning completed successfully! Cleaned data saved to cleaned_makeup.csv\nüéâ The complete data cleaning pipeline finished successfully! Original data: makeup.csv\nCleaned data: cleaned_makeup.csv\nDATA SUMMARY:\nRows: 1006\nColumns: 13\nMissing values: 145"
  },
  {
    "objectID": "agent.html#footnotes",
    "href": "agent.html#footnotes",
    "title": "Multi-Agent System",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/imas/IMAS2e.html‚Ü©Ô∏é\nhttps://www.ibm.com/think/topics/multiagent-system‚Ü©Ô∏é\nhttps://platform.openai.com/docs/overview‚Ü©Ô∏é"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion & Future Improvement",
    "section": "",
    "text": "This project develops an AI-enhanced data engineering system for e-commerce datasets, aiming to automate format detection, schema inference, cleaning, transformation, and storage of messy real-world data from platforms like Amazon, eBay, and Shopify. It uses RAG technique to let LLM retrieve example cleaning process from vector databse and collaborate with three-agent architecture.\nRetrieval and Prompt Generation Agent: Detects file format, extracts schema, queries a FAISS-based knowledge base, and generates structured prompts.\nCode Generation Agent: Generates Python cleaning code using LLMs (GPT-4o).\nCode Execution Agent: Runs the generated code, handles errors, and ensures successful data cleaning.\nThe system trains on real, noisy, and multi-format datasets and simulates real-world scenarios where schemas vary significantly. By combining Retrieval-Augmented Generation (RAG) techniques and LLM-driven code generation, the project aims to substantially reduce manual effort in preparing e-commerce data for analytics and machine learning tasks.\n\n\n\n\nIncreased scalability and speed of e-commerce data pipelines.\nClean the data to visualization ready status Effectively.\nProvide Higher accuracy and consistency in data cleaning across varied formats and messy schemas.\nImproved productivity for analysts and ML engineers working with large, heterogeneous datasets.\n\n\n\n\n\nModel Dependence: Success heavily depends on the quality of prompts and the performance of LLMs (GPT-4o) at each time‚Äôs generation, which may not always generate perfect code for complex or unseen schemas.\nLimited Error Handling: The current re-generation loop in the Code Execution Agent might not handle deeply nested errors or corner cases (e.g., nested JSON/XML structures) gracefully. Also, the it might cost time to regenerate code.\nSchema Generalization: Matching new data to historical schemas via FAISS works well if the database is rich, but performance may degrade if encountering completely novel schemas.\nEvaluation Subjectivity: The evaluation process might be ambiguous by the agent evaluation\nTest Data Bias: Testing on the different datasets may have different test results and not all corner cases may not considered.\n\n\n\n\n\nError Categorization and Dynamic Recovery 1 : Instead of simple retrying, classify the type of code failure and adjust prompt templates dynamically based on error type (e.g., missing column, data type mismatch).\n\n\nFine-Tuning: Fine-tune the LLM specifically on e-commerce transformation tasks for better contextual understanding and more reliable code generation.\nSchema Evolution Handling 2 : Introduce automatic schema version tracking, so if source platforms evolve (e.g., Amazon changes product attributes), the system can adapt.\n\n\nFeedback Loop with Reinforcement Learning: Implement a reinforcement learning (RL) feedback loop where better cleaning outcomes (e.g., fewer missing values, consistent schemas) are rewarded, improving agent performance over time.\n**Extend to Semi-Structured and Unstructured Data: Expand capability beyond strictly tabular formats to include semi-structured (HTML product pages) or unstructured text (user reviews, comments) and specifically focusing on improving the model‚Äôs performance on complex file formats.\nAdd A Multi-Agent Coordination 3 : Introduce a coordinator agent that dynamically assigns task priorities and supervise the process(e.g., prioritize missing value imputation before type conversion if data is highly incomplete)."
  },
  {
    "objectID": "conclusion.html#summary",
    "href": "conclusion.html#summary",
    "title": "Conclusion & Future Improvement",
    "section": "",
    "text": "This project develops an AI-enhanced data engineering system for e-commerce datasets, aiming to automate format detection, schema inference, cleaning, transformation, and storage of messy real-world data from platforms like Amazon, eBay, and Shopify. It uses RAG technique to let LLM retrieve example cleaning process from vector databse and collaborate with three-agent architecture.\nRetrieval and Prompt Generation Agent: Detects file format, extracts schema, queries a FAISS-based knowledge base, and generates structured prompts.\nCode Generation Agent: Generates Python cleaning code using LLMs (GPT-4o).\nCode Execution Agent: Runs the generated code, handles errors, and ensures successful data cleaning.\nThe system trains on real, noisy, and multi-format datasets and simulates real-world scenarios where schemas vary significantly. By combining Retrieval-Augmented Generation (RAG) techniques and LLM-driven code generation, the project aims to substantially reduce manual effort in preparing e-commerce data for analytics and machine learning tasks."
  },
  {
    "objectID": "conclusion.html#impact",
    "href": "conclusion.html#impact",
    "title": "Conclusion & Future Improvement",
    "section": "",
    "text": "Increased scalability and speed of e-commerce data pipelines.\nClean the data to visualization ready status Effectively.\nProvide Higher accuracy and consistency in data cleaning across varied formats and messy schemas.\nImproved productivity for analysts and ML engineers working with large, heterogeneous datasets."
  },
  {
    "objectID": "conclusion.html#limitations-of-the-current-implementation",
    "href": "conclusion.html#limitations-of-the-current-implementation",
    "title": "Conclusion & Future Improvement",
    "section": "",
    "text": "Model Dependence: Success heavily depends on the quality of prompts and the performance of LLMs (GPT-4o) at each time‚Äôs generation, which may not always generate perfect code for complex or unseen schemas.\nLimited Error Handling: The current re-generation loop in the Code Execution Agent might not handle deeply nested errors or corner cases (e.g., nested JSON/XML structures) gracefully. Also, the it might cost time to regenerate code.\nSchema Generalization: Matching new data to historical schemas via FAISS works well if the database is rich, but performance may degrade if encountering completely novel schemas.\nEvaluation Subjectivity: The evaluation process might be ambiguous by the agent evaluation\nTest Data Bias: Testing on the different datasets may have different test results and not all corner cases may not considered."
  },
  {
    "objectID": "conclusion.html#potential-future-improvements-and-extensions",
    "href": "conclusion.html#potential-future-improvements-and-extensions",
    "title": "Conclusion & Future Improvement",
    "section": "",
    "text": "Error Categorization and Dynamic Recovery 1 : Instead of simple retrying, classify the type of code failure and adjust prompt templates dynamically based on error type (e.g., missing column, data type mismatch).\n\n\nFine-Tuning: Fine-tune the LLM specifically on e-commerce transformation tasks for better contextual understanding and more reliable code generation.\nSchema Evolution Handling 2 : Introduce automatic schema version tracking, so if source platforms evolve (e.g., Amazon changes product attributes), the system can adapt.\n\n\nFeedback Loop with Reinforcement Learning: Implement a reinforcement learning (RL) feedback loop where better cleaning outcomes (e.g., fewer missing values, consistent schemas) are rewarded, improving agent performance over time.\n**Extend to Semi-Structured and Unstructured Data: Expand capability beyond strictly tabular formats to include semi-structured (HTML product pages) or unstructured text (user reviews, comments) and specifically focusing on improving the model‚Äôs performance on complex file formats.\nAdd A Multi-Agent Coordination 3 : Introduce a coordinator agent that dynamically assigns task priorities and supervise the process(e.g., prioritize missing value imputation before type conversion if data is highly incomplete)."
  },
  {
    "objectID": "conclusion.html#footnotes",
    "href": "conclusion.html#footnotes",
    "title": "Conclusion & Future Improvement",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00660/120911/Automatically-Correcting-Large-Language-Models‚Ü©Ô∏é\nhttps://web.eecs.umich.edu/~mozafari/winter2014/eecs684/papers/prism.pdf‚Ü©Ô∏é\nhttps://integrail.ai/blog/multi-agent-ai-coordination‚Ü©Ô∏é"
  }
]